{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import csv \n",
    "import sys\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import json\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sr20.csv', header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cleaning rows with null body\n",
    "df = df[df.selftext.notnull()]\n",
    "\n",
    "# clean out comments which were deleted\n",
    "df = df[df.selftext != '[deleted]']\n",
    "\n",
    "# select text only posts\n",
    "df = df[df.apply(lambda r: r.domain[:5] == 'self.', axis=1)]\n",
    "\n",
    "\n",
    "all_words = []\n",
    "def clean_body(row):\n",
    "    # clean unprintable chars\n",
    "    body = filter(lambda x: x in string.printable, row.selftext)\n",
    "    # maybe we dont want to just replace it with space. some words might have an apost in it and this \n",
    "    # will just split those words. but maybe these words are not that imp either?\n",
    "    body = re.sub(\"[^a-zA-Z]\", \" \", body ).lower()\n",
    "    \n",
    "    all_words.extend(word_tokenize(body))\n",
    "    return body\n",
    "\n",
    "df['body'] = df.apply(clean_body, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeWordCounts(all_words):\n",
    "    wordCounts = {}\n",
    "    for word in all_words:\n",
    "        if word in wordCounts:\n",
    "            wordCounts[word] += 1 \n",
    "        else: \n",
    "            wordCounts[word] = 1\n",
    "    return wordCounts\n",
    "\n",
    "wordCounts = computeWordCounts(all_words)\n",
    "sortedWords = sorted(wordCounts, key=wordCounts.get, reverse=True)\n",
    "mostFreqWords = sortedWords[0:1000]\n",
    "freqWordCounts = [wordCounts[word] for word in mostFreqWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_n_frequent_words(df, n):\n",
    "    return pd.Series(' '.join(df['body']).lower().split()).value_counts()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(df)\n",
    "df.subreddit.value_counts() #* 100.0/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped news due to no high scoring posts in test data\n",
      "Skipped animenews due to no high scoring posts in test data\n",
      "Skipped betternews due to no training or testing instances\n"
     ]
    }
   ],
   "source": [
    "#cls = 'positive'\n",
    "#cls = 'score_class'\n",
    "#cls = 'score_class_subreddit'\n",
    "#cls = 'subreddit'\n",
    "#cls = 'subreddit_implicit'\n",
    "cls = 'our_model'\n",
    "\n",
    "def get_score_class(row):\n",
    "    score = row.score\n",
    "    if score < 10:\n",
    "        return 1\n",
    "    if score < 100:\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "if cls == 'positive':\n",
    "    df[cls] =  df.apply(lambda r: r.score > 0, axis=1)\n",
    "elif cls == 'score_class' or cls == 'subreddit_implicit' or cls == 'score_class_subreddit' or cls == 'our_model':\n",
    "    df[cls] =  df.apply(get_score_class, axis=1)\n",
    "    \n",
    "# if its baseline model eval or our model, we want to have the subreddit and the corresponding class labels together\n",
    "if cls == 'subreddit_implicit' or cls == 'our_model':\n",
    "    df[cls] = zip(df[cls], df.subreddit)\n",
    "\n",
    "\n",
    "if cls == 'score_class_subreddit':\n",
    "    sr_split = {}\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", max_features = 1000, stop_words='english')\n",
    "    #vectorizer = TfidfVectorizer(max_df=0.5, max_features=1000, min_df=2, stop_words='english')\n",
    "    train_data_features_all = vectorizer.fit_transform(df.body.values)\n",
    "    train_data_features_all = train_data_features_all.toarray()\n",
    "    for sr in df.subreddit.unique():\n",
    "        bool_idx = (df.subreddit == sr).values\n",
    "        data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    train_data_features_all[bool_idx], df[cls].values[bool_idx], test_size=0.33, random_state=1121)\n",
    "        if len(data_train) == 0 or len(data_test) == 0:\n",
    "            continue\n",
    "        sr_split[sr] = data_train, data_test, labels_train, labels_test\n",
    "elif cls == 'our_model':\n",
    "    sr_train = {}\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", max_features = 1000, stop_words='english')\n",
    "    #vectorizer = TfidfVectorizer(max_df=0.5, max_features=1000, min_df=2, stop_words='english')\n",
    "    train_data_features_all = vectorizer.fit_transform(df.body.values)\n",
    "    train_data_features_all = train_data_features_all.toarray()\n",
    "    data_train_agg = []\n",
    "    labels_train_agg = []\n",
    "    data_test_agg = []\n",
    "    labels_test_agg = []\n",
    "    for subr in df.subreddit.unique():\n",
    "        bool_idx = (df.subreddit == subr).values\n",
    "        sr_data_train, sr_data_test, sr_labels_train, sr_labels_test = train_test_split(\n",
    "    train_data_features_all[bool_idx], df[cls].values[bool_idx], test_size=0.33, random_state=1121)\n",
    "        #print subr, len(sr_data_train), len(sr_data_test)\n",
    "        \n",
    "        if len(sr_data_train) == 0 or len(sr_data_test) == 0:\n",
    "            print 'Skipped {0} due to no training or testing instances'.format(subr)\n",
    "            continue\n",
    "        \n",
    "        # need for training subreddit classifiers. here labels should be score classes\n",
    "        sr_train[subr] = (sr_data_train, [score_class for (score_class, sr) in sr_labels_train])\n",
    "        \n",
    "        # need for training base subreddit classifer. here labels should be subreddits\n",
    "        data_train_agg += list(sr_data_train)\n",
    "        labels_train_agg += [sr for (score_class, sr) in sr_labels_train]\n",
    "        \n",
    "        # for the testing model, we only want insatnces of high score posts\n",
    "        test_instances = zip(sr_data_test, sr_labels_test)\n",
    "        #print subr, len(sr_data_train), len(sr_data_test), test_instances[0]\n",
    "        test_instances = [(fv, sr) for fv, (score_class, sr) in test_instances if score_class == 2 ]\n",
    "        \n",
    "        if len(test_instances) == 0:\n",
    "            print 'Skipped {0} due to no high scoring posts in test data'.format(subr)\n",
    "            continue\n",
    "        #print subr, len(sr_data_train), len(sr_data_test), test_instances[0]\n",
    "        sr_data_test, sr_labels_test = zip(*test_instances)\n",
    "    \n",
    "        # need for testing combined model. here instances are high scoring posts and labels should be subreddits \n",
    "        data_test_agg += sr_data_test\n",
    "        labels_test_agg += sr_labels_test\n",
    "        \n",
    "        \n",
    "else:\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", max_features = 1000, stop_words='english')\n",
    "    #vectorizer = TfidfVectorizer(max_df=0.5, max_features=1000, min_df=2, stop_words='english')\n",
    "    train_data_features = vectorizer.fit_transform(df.body.values)\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    train_data_features, df[cls].values, test_size=0.33, random_state=1121)\n",
    "\n",
    "# if its implicit eval of baseline model, for the testing rows, we only want insatnces of high score posts\n",
    "if cls == 'subreddit_implicit':\n",
    "    test_instances = zip(data_test, labels_test)\n",
    "    test_instances = [(fv, sr) for fv, (score_class, sr) in test_instances if score_class == 2 ]\n",
    "    data_test, labels_test = zip(*test_instances)\n",
    "    labels_train = [sr for (score_class, sr) in labels_train]\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering data with KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=5, n_init=10,\n",
      "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
      "    verbose=0)\n",
      "[0 1 3 3 0 2 2 4 0 1 2 2 1 1 1 1]\n",
      "0 set(['relationships', 'Divorce', 'Marriage'])\n",
      "1 set(['TrueAnime', 'anime', 'news', 'nba', 'NBA_Draft', 'manga'])\n",
      "2 set(['weightroom', 'weightlifting', 'Stronglifts5x5', 'Fitness'])\n",
      "3 set(['fantasybball', 'animenews'])\n",
      "4 set(['inthenews'])\n"
     ]
    }
   ],
   "source": [
    "if cls == 'our_model':\n",
    "    allCounts = np.zeros(shape=(len(mostFreqWords) ,len(sr_train)))\n",
    "    subInd = 0\n",
    "    sr_map = {}\n",
    "    for sr in sr_train: \n",
    "\n",
    "        # get posts \n",
    "        posts = df[df.subreddit == sr].body\n",
    "\n",
    "        # convert to all words\n",
    "        allPosts = \" \".join(posts)\n",
    "        allWords = allPosts.split()\n",
    "        allWords = [word.lower() for word in allWords]\n",
    "\n",
    "        # count words \n",
    "        wordCounts = computeWordCounts(allWords)\n",
    "\n",
    "        # get total words \n",
    "        totWords = float(len(allWords))\n",
    "        #print totWords\n",
    "\n",
    "        # get freqWordCounts \n",
    "        freqWordCounts = [wordCounts[word] if word in wordCounts else 0 for word in mostFreqWords ]\n",
    "        freqWordCounts = [i/totWords for i in freqWordCounts]\n",
    "\n",
    "        # concatenate \n",
    "        allCounts[:,subInd] = freqWordCounts\n",
    "\n",
    "        sr_map[subInd] = sr\n",
    "        subInd += 1\n",
    "        \n",
    "        \n",
    "\"\"\" get pairwise distance \"\"\"\n",
    "normMax = np.max(allCounts, axis=1)\n",
    "normCounts = np.transpose(np.divide(np.transpose(allCounts), normMax))\n",
    "distVec = spatial.distance.pdist(np.transpose(normCounts), 'euclidean')\n",
    "distMat = spatial.distance.squareform(distVec)\n",
    "\n",
    "normCounts = np.transpose(normCounts)\n",
    "\n",
    "n = 5\n",
    "km = KMeans(n_clusters=n, init='k-means++', max_iter=100, n_init=10)               \n",
    "\n",
    "print(\"Clustering data with %s\" % km)\n",
    "km.fit(normCounts)\n",
    "print(km.labels_)\n",
    "\n",
    "cluster = {i:set() for i in range(n)}\n",
    "sr_cluster = {}\n",
    "for idx in range(len(km.labels_)):\n",
    "    cluster[km.labels_[idx]].add(sr_map[idx])\n",
    "    sr_cluster[sr_map[idx]] = km.labels_[idx]\n",
    "for label, st in cluster.iteritems():\n",
    "    print label, st\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Fitness') (1, 'nba') (1, 'news') (1, 'relationships') (2, 'Fitness')\n",
      " (2, 'relationships') (1, 'manga') (2, 'nba') (1, 'anime') (2, 'manga')\n",
      " (2, 'anime') (3, 'relationships') (1, 'Stronglifts5x5') (1, 'Divorce')\n",
      " (3, 'Fitness') (2, 'Marriage') (2, 'weightlifting') (2, 'weightroom')\n",
      " (2, 'fantasybball') (3, 'nba') (1, 'weightlifting') (3, 'anime')\n",
      " (2, 'Divorce') (1, 'animenews') (1, 'Marriage') (2, 'Stronglifts5x5')\n",
      " (1, 'TrueAnime') (2, 'inthenews') (1, 'inthenews') (1, 'weightroom')\n",
      " (2, 'TrueAnime') (1, 'NBA_Draft') (3, 'weightroom') (3, 'manga')\n",
      " (2, 'NBA_Draft') (1, 'fantasybball') (1, 'betternews') (3, 'news')\n",
      " (3, 'inthenews')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, relationships)     31.063048\n",
       "(1, Fitness)           24.297174\n",
       "(1, nba)                8.988680\n",
       "(2, relationships)      6.064922\n",
       "(1, news)               5.926231\n",
       "(1, anime)              4.633031\n",
       "(2, anime)              2.912512\n",
       "(1, manga)              2.571407\n",
       "(2, nba)                2.413974\n",
       "(3, relationships)      1.990404\n",
       "(2, Fitness)            1.668041\n",
       "(3, nba)                1.315691\n",
       "(1, Divorce)            1.117025\n",
       "(3, anime)              1.109528\n",
       "(1, weightlifting)      0.940850\n",
       "(2, manga)              0.753430\n",
       "(3, Fitness)            0.502287\n",
       "(1, Stronglifts5x5)     0.386086\n",
       "(1, Marriage)           0.266137\n",
       "(2, TrueAnime)          0.138691\n",
       "(2, Marriage)           0.127446\n",
       "(2, weightroom)         0.108704\n",
       "(1, NBA_Draft)          0.104955\n",
       "(2, Divorce)            0.104955\n",
       "(1, inthenews)          0.089962\n",
       "(1, TrueAnime)          0.086213\n",
       "(2, weightlifting)      0.074968\n",
       "(2, NBA_Draft)          0.052478\n",
       "(2, Stronglifts5x5)     0.048729\n",
       "(1, weightroom)         0.037484\n",
       "(1, fantasybball)       0.037484\n",
       "(3, manga)              0.018742\n",
       "(2, inthenews)          0.011245\n",
       "(2, fantasybball)       0.011245\n",
       "(1, animenews)          0.007497\n",
       "(3, weightroom)         0.007497\n",
       "(3, inthenews)          0.003748\n",
       "(3, news)               0.003748\n",
       "(1, betternews)         0.003748\n",
       "dtype: float64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df[cls].unique()\n",
    "df[cls].value_counts() * 100.0 /len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if cls == 'positive':\n",
    "    all_classes = np.array([False, True])\n",
    "elif cls == 'score_class' or cls == 'score_class_subreddit':\n",
    "    all_classes = np.array([1, 2, 3])\n",
    "elif cls == 'subreddit' or cls == 'subreddit_implicit':\n",
    "    all_classes = np.array(['Fitness', 'nba', 'news', 'relationships', 'manga', 'anime', 'Stronglifts5x5',\n",
    " 'Divorce', 'Marriage', 'weightlifting', 'weightroom', 'fantasybball',\n",
    " 'animenews', 'TrueAnime', 'inthenews', 'NBA_Draft', 'betternews'])\n",
    "\n",
    "\n",
    "if cls == 'our_model':\n",
    "    # we need to train 2 models. subreddit classifier and the inndivual subreddit scorers\n",
    "    subreddit_model = MultinomialNB(alpha=0.01)\n",
    "    #labels_train_agg = [sr_cluster[sr] for sr in labels_train_agg]\n",
    "    subreddit_model.fit(data_train_agg, labels_train_agg)\n",
    "    \n",
    "    sr_model = {}\n",
    "    for sr, (data_train, labels_train) in sr_train.iteritems():\n",
    "        model = MultinomialNB(alpha=0.01)\n",
    "        model.fit(data_train, labels_train)\n",
    "        sr_model[sr] = model\n",
    "        \n",
    "\n",
    "elif cls == 'score_class_subreddit':\n",
    "    sr_model = {}\n",
    "    for sr, split in sr_split.iteritems():\n",
    "        data_train, data_test, labels_train, labels_test = split\n",
    "        model = MultinomialNB(alpha=0.01)\n",
    "        model.fit(data_train, labels_train)\n",
    "        sr_model[sr] = model\n",
    "else:\n",
    "    model = MultinomialNB(alpha=0.01)\n",
    "    #model = SGDClassifier()\n",
    "\n",
    "    training = 'offline'\n",
    "    if training == 'online':\n",
    "        slice_size = len(train_data_features)/1000\n",
    "        for start in range(0, len(train_data_features), slice_size):\n",
    "            slice_train_data = data_train[start:start+slice_size]\n",
    "            slice_label_data = labels_train[start:start+slice_size]\n",
    "            model.partial_fit(slice_train_data, slice_label_data, classes=all_classes)\n",
    "    else:\n",
    "        model.fit(data_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.5944272446\n"
     ]
    }
   ],
   "source": [
    "def our_model(subreddit_model, cluster, sr_model, data_test_agg):\n",
    "    predicted = []\n",
    "    for f in data_test_agg:\n",
    "        label = subreddit_model.predict([f])[0]\n",
    "#         sr = subreddit_model.predict([f])[0]\n",
    "#         for label, st in cluster.iteritems():\n",
    "#             if sr in st:\n",
    "#                 break\n",
    "        best_sr = ''\n",
    "        best_score = 0\n",
    "        for sr in cluster[label]:\n",
    "            score = sr_model[sr].predict([f])[0]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_sr = sr\n",
    "                \n",
    "        predicted.append(best_sr)\n",
    "    return predicted\n",
    "                \n",
    "\n",
    "if cls == 'our_model':\n",
    "    predicted = our_model(subreddit_model, cluster, sr_model, data_test_agg)\n",
    "    correct = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == labels_test_agg[i]:\n",
    "            correct += 1\n",
    "    print correct * 100.0 / len(predicted)\n",
    "elif cls == 'score_class_subreddit':\n",
    "    for sr, split in sr_split.iteritems():\n",
    "        data_train, data_test, labels_train, expected = split\n",
    "        model = sr_model[sr]\n",
    "        predicted = model.predict(data_test)\n",
    "        print 'Results fro subreddit classifier for ' + sr\n",
    "        print(metrics.classification_report(expected, predicted))\n",
    "        print(metrics.confusion_matrix(expected, predicted))\n",
    "else:\n",
    "    expected = labels_test\n",
    "    predicted = model.predict(data_test)\n",
    "    if cls == 'subreddit_implicit':\n",
    "        correct = 0\n",
    "        for i in range(len(predicted)):\n",
    "            if predicted[i] == expected[i]:\n",
    "                correct += 1\n",
    "        print correct * 100.0 / len(predicted)\n",
    "    else:\n",
    "        # summarize the fit of the model\n",
    "        print(metrics.classification_report(expected, predicted))\n",
    "        print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.8823529412\n"
     ]
    }
   ],
   "source": [
    "#x = [sr_cluster[sr] for sr in labels_test_agg]\n",
    "x = labels_test_agg\n",
    "p = subreddit_model.predict(data_test_agg)\n",
    "correct = 0\n",
    "for i in range(len(predicted)):\n",
    "    if p[i] == x[i]:\n",
    "        correct += 1\n",
    "print correct * 100.0 / len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_n_frequent_words(df, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "for word in vectorizer.get_feature_names():\n",
    "    word_count[word] = all_words.count(word)\n",
    "\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 10, stop_words='english')\n",
    "#vectorizer = TfidfVectorizer(max_df=0.5, max_features=1000, min_df=2, stop_words='english')\n",
    "x = vectorizer.fit_transform(['i am just testing how this thing', 'works. whether or not', 'it stores state every time i do', 'a fit transform'])\n",
    "\n",
    "print vectorizer.get_feature_names()\n",
    "print x.toarray()\n",
    "x = vectorizer.fit_transform(['fit fit fit fit', 'works. every every every or not', 'it stores state every time i do', 'a fit transform'])\n",
    "print vectorizer.get_feature_names()\n",
    "print x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB(alpha=0.01)\n",
    "nb_model.fit(data_train, labels_train)\n",
    "\n",
    "expected = labels_test\n",
    "predicted = nb_model.predict(data_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "df['train'] = zip(df.body, df.positive)\n",
    "cl = NaiveBayesClassifier(df.train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = {}\n",
    "for w in all_words:\n",
    "    count[w] = count.setdefault(w,0) + 1\n",
    "\n",
    "features = sorted(count, key=count.get, reverse=True)[:2000]\n",
    "feature_map = {features[i]:i for i in range(len(features))}\n",
    "\n",
    "def generate_training_tuple(row):\n",
    "    words = set(row.body.split())\n",
    "    features_set = {}\n",
    "    for feature in features:\n",
    "        features_set[feature_map[feature]] = (feature in words)\n",
    "    return features_set, row.positive\n",
    "\n",
    "data = df.apply(generate_training_tuple, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s='''news\n",
    "inthenews\n",
    "worldnews\n",
    "betternews\n",
    "Fitness\n",
    "weightlifting\n",
    "weightroom\n",
    "Stronglifts5x5\n",
    "relationships\n",
    "Divorce\n",
    "Marriage\n",
    "Dating\n",
    "nba\n",
    "NBA_Draft\n",
    "fantasybball\n",
    "wtfnba\n",
    "anime\n",
    "animenews\n",
    "TrueAnime\n",
    "manga'''\n",
    "for sr in s.split('\\n'):\n",
    "    print 'select count(*) from posts where subreddit=\"' + sr + '\";'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s='''news\n",
    "inthenews\n",
    "worldnews\n",
    "betternews\n",
    "fitness\n",
    "weightlifting\n",
    "weightroom\n",
    "Stronglifts5x5\n",
    "relationships\n",
    "Divorce\n",
    "Marriage\n",
    "Dating\n",
    "nba\n",
    "NBA_Draft\n",
    "fantasybball\n",
    "wtfnba\n",
    "anime\n",
    "animenews\n",
    "TrueAnime\n",
    "manga'''\n",
    "\"' or subreddit='\".join(s.split('\\n'))\n",
    "\n",
    "################################################\n",
    "# OLDER CLEAN CODE WIHTOUT LEMMATIZATION\n",
    "# cleaning rows with null body\n",
    "df = df[df.selftext.notnull()]\n",
    "\n",
    "# clean out comments which were deleted\n",
    "df = df[df.selftext != '[deleted]']\n",
    "\n",
    "# select text only posts\n",
    "df = df[df.apply(lambda r: r.domain[:5] == 'self.', axis=1)]\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "all_words = []\n",
    "def clean_body(row):\n",
    "    # clean unprintable chars\n",
    "    body = filter(lambda x: x in string.printable, row.selftext)\n",
    "    body = re.sub(\"[^a-zA-Z]\", \" \", body )\n",
    "    words = body.lower().split()   \n",
    "    # remove stop words\n",
    "    words = [w for w in words if (w not in stop_words) and (len(w) > 3)]\n",
    "    all_words.extend(words)\n",
    "    return \" \".join( words )\n",
    "\n",
    "df['body'] = df.apply(clean_body, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "# code for extracting features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 1000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(df.body.values)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "\n",
    "\n",
    "# top 20 words with this method\n",
    "like      41581\n",
    "would     31031\n",
    "don't     27578\n",
    "know      26772\n",
    "really    26746\n",
    "want      23404\n",
    "time      21831\n",
    "even      18897\n",
    "it's      18772\n",
    "feel      18115\n",
    "game      17866\n",
    "think     17726\n",
    "i've      15449\n",
    "going     15015\n",
    "people    14642\n",
    "still     14501\n",
    "said      13889\n",
    "could     13800\n",
    "never     13721\n",
    "much      13622\n",
    "\n",
    "\n",
    "#### results for topic modelling with most 1000 tf-idf features\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "    Divorce       0.80      0.04      0.07       102\n",
    "    Fitness       0.89      0.94      0.91      2348\n",
    "   Marriage       0.00      0.00      0.00        29\n",
    "  NBA_Draft       0.00      0.00      0.00        18\n",
    "Stronglifts5x5       0.00      0.00      0.00        29\n",
    "  TrueAnime       0.00      0.00      0.00        23\n",
    "      anime       0.80      0.74      0.77       780\n",
    "  animenews       0.00      0.00      0.00         1\n",
    " betternews       0.00      0.00      0.00         1\n",
    "fantasybball       0.00      0.00      0.00         7\n",
    "  inthenews       0.00      0.00      0.00         8\n",
    "      manga       0.91      0.50      0.65       318\n",
    "        nba       0.90      0.75      0.82      1085\n",
    "       news       0.86      0.48      0.62       526\n",
    "relationships       0.81      1.00      0.89      3424\n",
    "weightlifting       1.00      0.07      0.14        94\n",
    " weightroom       1.00      0.09      0.17        11\n",
    "\n",
    "avg / total       0.84      0.84      0.82      8804\n",
    "\n",
    "\n",
    "#### results for topic modelling with 1000 most frequent words\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "    Divorce       0.41      0.63      0.50       102\n",
    "    Fitness       0.91      0.87      0.89      2348\n",
    "   Marriage       0.22      0.17      0.19        29\n",
    "  NBA_Draft       0.18      0.11      0.14        18\n",
    "Stronglifts5x5       0.03      0.07      0.04        29\n",
    "  TrueAnime       0.58      0.65      0.61        23\n",
    "      anime       0.82      0.60      0.69       780\n",
    "  animenews       0.00      0.00      0.00         1\n",
    " betternews       0.00      0.00      0.00         1\n",
    "fantasybball       0.00      0.00      0.00         7\n",
    "  inthenews       0.25      0.12      0.17         8\n",
    "      manga       0.58      0.74      0.65       318\n",
    "        nba       0.86      0.76      0.81      1085\n",
    "       news       0.83      0.61      0.70       526\n",
    "relationships       0.86      0.97      0.91      3424\n",
    "weightlifting       0.37      0.37      0.37        94\n",
    " weightroom       0.14      0.27      0.18        11\n",
    "\n",
    "avg / total       0.84      0.83      0.83      8804\n",
    "\n",
    "\n",
    "#######################################\n",
    "##### SCORE CLSSIFER WITH 1000 TF IDF\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.82      0.99      0.90      7079\n",
    "          2       0.61      0.07      0.12      1287\n",
    "          3       0.44      0.14      0.21       438\n",
    "\n",
    "avg / total       0.77      0.81      0.75      8804\n",
    "\n",
    "[[7018   39   22]\n",
    " [1144   85   58]\n",
    " [ 361   15   62]]\n",
    "\n",
    "### WITH 100 TFIDF FEATURES\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.81      1.00      0.89      7079\n",
    "          2       0.69      0.02      0.05      1287\n",
    "          3       0.00      0.00      0.00       438\n",
    "\n",
    "avg / total       0.75      0.81      0.73      8804\n",
    "\n",
    "[[7071    6    2]\n",
    " [1249   31    7]\n",
    " [ 430    8    0]]\n",
    "\n",
    "\n",
    "###### WITH MOST FREQUENT 1000 WORDS\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.85      0.86      0.86      7079\n",
    "          2       0.29      0.23      0.26      1287\n",
    "          3       0.26      0.37      0.30       438\n",
    "\n",
    "avg / total       0.74      0.74      0.74      8804\n",
    "\n",
    "[[6077  676  326]\n",
    " [ 840  296  151]\n",
    " [ 214   60  164]]\n",
    "\n",
    "# MOST FREQUENT 100 WORDS\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.84      0.89      0.86      7079\n",
    "          2       0.25      0.20      0.22      1287\n",
    "          3       0.33      0.18      0.24       438\n",
    "\n",
    "avg / total       0.73      0.75      0.74      8804\n",
    "\n",
    "[[6308  676   95]\n",
    " [ 961  259   67]\n",
    " [ 267   91   80]]\n",
    "\n",
    "\n",
    "# subreddit classifiers with diff features \n",
    "Results fro subreddit classifier for relationships\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.86      0.78      0.82      2735\n",
    "          2       0.22      0.27      0.24       535\n",
    "          3       0.30      0.53      0.38       174\n",
    "\n",
    "avg / total       0.73      0.69      0.71      3444\n",
    "\n",
    "[[2140  457  138]\n",
    " [ 307  144   84]\n",
    " [  38   43   93]]\n",
    "Results fro subreddit classifier for TrueAnime\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.75      0.33      0.46         9\n",
    "          2       0.62      0.91      0.74        11\n",
    "\n",
    "avg / total       0.68      0.65      0.62        20\n",
    "\n",
    "[[ 3  6]\n",
    " [ 1 10]]\n",
    "Results fro subreddit classifier for fantasybball\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.00      0.00      0.00         3\n",
    "          2       0.25      0.50      0.33         2\n",
    "\n",
    "avg / total       0.10      0.20      0.13         5\n",
    "\n",
    "[[0 3]\n",
    " [1 1]]\n",
    "Results fro subreddit classifier for animenews\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       1.00      1.00      1.00         1\n",
    "\n",
    "avg / total       1.00      1.00      1.00         1\n",
    "\n",
    "[[1]]\n",
    "Results fro subreddit classifier for Divorce\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.94      1.00      0.97       102\n",
    "          2       0.00      0.00      0.00         6\n",
    "\n",
    "avg / total       0.89      0.94      0.92       108\n",
    "\n",
    "[[102   0]\n",
    " [  6   0]]\n",
    "Results fro subreddit classifier for weightroom\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       1.00      0.17      0.29         6\n",
    "          2       0.54      1.00      0.70         7\n",
    "          3       0.00      0.00      0.00         1\n",
    "\n",
    "avg / total       0.70      0.57      0.47        14\n",
    "\n",
    "[[1 5 0]\n",
    " [0 7 0]\n",
    " [0 1 0]]\n",
    "Results fro subreddit classifier for Stronglifts5x5\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.89      0.94      0.92        35\n",
    "          2       0.00      0.00      0.00         4\n",
    "\n",
    "avg / total       0.80      0.85      0.82        39\n",
    "\n",
    "[[33  2]\n",
    " [ 4  0]]\n",
    "Results fro subreddit classifier for inthenews\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.86      0.75      0.80         8\n",
    "          2       0.33      1.00      0.50         1\n",
    "          3       0.00      0.00      0.00         1\n",
    "\n",
    "avg / total       0.72      0.70      0.69        10\n",
    "\n",
    "[[6 2 0]\n",
    " [0 1 0]\n",
    " [1 0 0]]\n",
    "Results fro subreddit classifier for Marriage\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.68      1.00      0.81        23\n",
    "          2       1.00      0.08      0.15        12\n",
    "\n",
    "avg / total       0.79      0.69      0.58        35\n",
    "\n",
    "[[23  0]\n",
    " [11  1]]\n",
    "Results fro subreddit classifier for anime\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.66      0.88      0.75       389\n",
    "          2       0.64      0.31      0.41       272\n",
    "          3       0.46      0.50      0.48       101\n",
    "\n",
    "avg / total       0.62      0.63      0.60       762\n",
    "\n",
    "[[343  32  14]\n",
    " [143  83  46]\n",
    " [ 35  15  51]]\n",
    "Results fro subreddit classifier for Fitness\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.92      0.92      0.92      2124\n",
    "          2       0.08      0.09      0.09       148\n",
    "          3       0.16      0.12      0.14        59\n",
    "\n",
    "avg / total       0.85      0.85      0.85      2331\n",
    "\n",
    "[[1961  131   32]\n",
    " [ 129   14    5]\n",
    " [  31   21    7]]\n",
    "Results fro subreddit classifier for weightlifting\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.93      0.96      0.95        84\n",
    "          2       0.00      0.00      0.00         6\n",
    "\n",
    "avg / total       0.87      0.90      0.88        90\n",
    "\n",
    "[[81  3]\n",
    " [ 6  0]]\n",
    "Results fro subreddit classifier for news\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       1.00      0.98      0.99       523\n",
    "          3       0.00      0.00      0.00         0\n",
    "\n",
    "avg / total       1.00      0.98      0.99       523\n",
    "\n",
    "[[513  10]\n",
    " [  0   0]]\n",
    "Results fro subreddit classifier for nba\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.74      0.81      0.77       790\n",
    "          2       0.23      0.17      0.19       222\n",
    "          3       0.35      0.29      0.31       108\n",
    "\n",
    "avg / total       0.60      0.63      0.61      1120\n",
    "\n",
    "[[641 108  41]\n",
    " [168  37  17]\n",
    " [ 60  17  31]]\n",
    "Results fro subreddit classifier for NBA_Draft\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.71      0.45      0.56        11\n",
    "          2       0.14      0.33      0.20         3\n",
    "\n",
    "avg / total       0.59      0.43      0.48        14\n",
    "\n",
    "[[5 6]\n",
    " [2 1]]\n",
    "Results fro subreddit classifier for manga\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.82      0.87      0.85       231\n",
    "          2       0.38      0.29      0.33        63\n",
    "          3       0.00      0.00      0.00         1\n",
    "\n",
    "avg / total       0.72      0.75      0.73       295\n",
    "\n",
    "[[202  29   0]\n",
    " [ 44  18   1]\n",
    " [  1   0   0]]\n",
    "\n",
    "\n",
    "##### RESULTS WITH SAME FEATURES FOR ALL SR CLASSIFIERS\n",
    "Results fro subreddit classifier for relationships\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.86      0.79      0.82      2735\n",
    "          2       0.22      0.25      0.23       535\n",
    "          3       0.30      0.53      0.38       174\n",
    "\n",
    "avg / total       0.73      0.69      0.71      3444\n",
    "\n",
    "[[2151  449  135]\n",
    " [ 314  136   85]\n",
    " [  40   41   93]]\n",
    "Results fro subreddit classifier for TrueAnime\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.75      0.33      0.46         9\n",
    "          2       0.62      0.91      0.74        11\n",
    "\n",
    "avg / total       0.68      0.65      0.62        20\n",
    "\n",
    "[[ 3  6]\n",
    " [ 1 10]]\n",
    "Results fro subreddit classifier for fantasybball\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.00      0.00      0.00         3\n",
    "          2       0.25      0.50      0.33         2\n",
    "\n",
    "avg / total       0.10      0.20      0.13         5\n",
    "\n",
    "[[0 3]\n",
    " [1 1]]\n",
    "Results fro subreddit classifier for animenews\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       1.00      1.00      1.00         1\n",
    "\n",
    "avg / total       1.00      1.00      1.00         1\n",
    "\n",
    "[[1]]\n",
    "Results fro subreddit classifier for Divorce\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.94      0.99      0.97       102\n",
    "          2       0.00      0.00      0.00         6\n",
    "\n",
    "avg / total       0.89      0.94      0.91       108\n",
    "\n",
    "[[101   1]\n",
    " [  6   0]]\n",
    "Results fro subreddit classifier for weightroom\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.00      0.00      0.00         6\n",
    "          2       0.50      1.00      0.67         7\n",
    "          3       0.00      0.00      0.00         1\n",
    "\n",
    "avg / total       0.25      0.50      0.33        14\n",
    "\n",
    "[[0 6 0]\n",
    " [0 7 0]\n",
    " [0 1 0]]\n",
    "Results fro subreddit classifier for Stronglifts5x5\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.89      0.97      0.93        35\n",
    "          2       0.00      0.00      0.00         4\n",
    "\n",
    "avg / total       0.80      0.87      0.84        39\n",
    "\n",
    "[[34  1]\n",
    " [ 4  0]]\n",
    "Results fro subreddit classifier for inthenews\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.80      0.50      0.62         8\n",
    "          2       0.20      1.00      0.33         1\n",
    "          3       0.00      0.00      0.00         1\n",
    "\n",
    "avg / total       0.66      0.50      0.53        10\n",
    "\n",
    "[[4 4 0]\n",
    " [0 1 0]\n",
    " [1 0 0]]\n",
    "Results fro subreddit classifier for Marriage\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.66      0.91      0.76        23\n",
    "          2       0.33      0.08      0.13        12\n",
    "\n",
    "avg / total       0.55      0.63      0.55        35\n",
    "\n",
    "[[21  2]\n",
    " [11  1]]\n",
    "Results fro subreddit classifier for anime\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.67      0.90      0.77       389\n",
    "          2       0.67      0.34      0.45       272\n",
    "          3       0.48      0.49      0.48       101\n",
    "\n",
    "avg / total       0.65      0.65      0.62       762\n",
    "\n",
    "[[350  27  12]\n",
    " [137  93  42]\n",
    " [ 34  18  49]]\n",
    "Results fro subreddit classifier for Fitness\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.92      0.93      0.93      2124\n",
    "          2       0.10      0.09      0.09       148\n",
    "          3       0.14      0.12      0.13        59\n",
    "\n",
    "avg / total       0.85      0.86      0.85      2331\n",
    "\n",
    "[[1984  103   37]\n",
    " [ 130   13    5]\n",
    " [  38   14    7]]\n",
    "Results fro subreddit classifier for weightlifting\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.93      1.00      0.97        84\n",
    "          2       0.00      0.00      0.00         6\n",
    "\n",
    "avg / total       0.87      0.93      0.90        90\n",
    "\n",
    "[[84  0]\n",
    " [ 6  0]]\n",
    "Results fro subreddit classifier for news\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       1.00      0.99      0.99       523\n",
    "          3       0.00      0.00      0.00         0\n",
    "\n",
    "avg / total       1.00      0.99      0.99       523\n",
    "\n",
    "[[516   7]\n",
    " [  0   0]]\n",
    "Results fro subreddit classifier for nba\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.74      0.86      0.79       790\n",
    "          2       0.21      0.11      0.14       222\n",
    "          3       0.42      0.37      0.39       108\n",
    "\n",
    "avg / total       0.61      0.66      0.63      1120\n",
    "\n",
    "[[676  78  36]\n",
    " [179  24  19]\n",
    " [ 57  11  40]]\n",
    "Results fro subreddit classifier for NBA_Draft\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.80      0.36      0.50        11\n",
    "          2       0.22      0.67      0.33         3\n",
    "\n",
    "avg / total       0.68      0.43      0.46        14\n",
    "\n",
    "[[4 7]\n",
    " [1 2]]\n",
    "Results fro subreddit classifier for manga\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.82      0.86      0.84       231\n",
    "          2       0.37      0.30      0.33        63\n",
    "          3       0.00      0.00      0.00         1\n",
    "\n",
    "avg / total       0.72      0.74      0.73       295\n",
    "\n",
    "[[199  32   0]\n",
    " [ 44  19   0]\n",
    " [  0   1   0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
